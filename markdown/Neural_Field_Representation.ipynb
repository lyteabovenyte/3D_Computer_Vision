{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2001f54d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".rendered_html {\n",
       "    font-family: Monaco, monospace;\n",
       "    font-size: 12px;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".rendered_html {\n",
    "    font-family: Monaco, monospace;\n",
    "    font-size: 12px;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1923b2b1",
   "metadata": {},
   "source": [
    "### Neural Field Representation:\n",
    "\n",
    "Mostly part of [this paper](https://arxiv.org/pdf/2107.04004)\n",
    "\n",
    "we proceed to consider the modelling and manipulation of\n",
    "dynamical systems with extremely high **degrees of freedom (DoF)**\n",
    "using only visual observations, such as manipulating a container of\n",
    "liquid with ice cubes floating within it (i.e., fluid-body interactions).\n",
    "In such a scenario, accurately estimating the full state information of\n",
    "the particle set becomes challenging when the sole inputs are RGB\n",
    "images. Moreover, the usage of *keypoints* (typically a sparse set of\n",
    "points attached to semantically meaningful parts of an object) in\n",
    "this context is uncertain since the fluid, possessing an extremely high\n",
    "DoF, continuously alters its shape during interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b18cd65",
   "metadata": {},
   "source": [
    "$?$ A central question in model learning for robotic manipulation is\n",
    "how to establish the state representation for learning the dynamics\n",
    "model? The ideal representation should readily capture the environmental dynamics, show a strong 3D understanding of the objects in the scene, and be applicable to a wide range of object sets including\n",
    "rigid or deformable objects and fluids. $?$\n",
    "\n",
    "Some methods, instead of estimating the state of the environment, *learn dynamics in the latent space*, However, the\n",
    "majority of these methods learn dynamics models using 2D convolutional neural networks and reconstruction loss, which is a similar issue to predicting dynamics in the image space, i.e., their learned\n",
    "representations lack equivariance to 3D transformations. **Time contrastive networks** aim to learn viewpoint-invariant representations from multi-view inputs but do not necessitate detailed modelling of 3D contents.\n",
    "\n",
    "here, we propose embedding **neural radiance fields**\n",
    "into an **autoencoder framework**, enabling tractable inference of the\n",
    "3D-structure-aware scene state for dynamic environments. By also\n",
    "enforcing a **time contrastive loss** on the estimated states, we ensure\n",
    "that the learned state representations are viewpoint-invariant. We\n",
    "then train a **dynamics model** that predicts the evolution of the state\n",
    "space conditioned on the input action, enabling control in the learned\n",
    "state space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0911324",
   "metadata": {},
   "source": [
    "#### More on NeRF itself:\n",
    "\n",
    "Neural Radiance Fields (NeRF) is a neural network architecture used for novel view synthesis and 3D scene reconstruction in computer vision. It represents a scene as a **continuous volumetric function** and allows generating highly realistic images of that scene from arbitrary viewpoints.\n",
    "\n",
    "At its core, NeRF is a fully connected neural network (usually a multi-layer perceptron, or MLP) that takes a 3D coordinate $\\mathbf{x} = (x, y, z)$ and a 2D viewing direction $\\mathbf{d} = (\\theta, \\phi)$, and outputs:\n",
    "- Color $\\mathbf{c} = (r, g, b)$\n",
    "- Volume density $\\sigma \\in \\mathbb{R}^{+}$\n",
    "\n",
    "This models the radiance field of the scene—essentially, how light and color behave at every point in space when viewed from a specific direction.\n",
    "\n",
    "How It Works (Under the Hood):\n",
    "\n",
    "NeRF uses volumetric rendering to synthesize an image. The process includes:\n",
    "1.\tRay Casting: For each pixel in the target image, cast a ray from the camera through that pixel into 3D space.\n",
    "2.\tSampling: Sample multiple points along each ray (stratified sampling).\n",
    "3.\tMLP Evaluation: For each sampled 3D point, feed ($\\mathbf{x}, \\mathbf{d}$) into the network to get ($\\mathbf{c}, \\sigma$).\n",
    "4.\tRendering Equation: Accumulate the colors and densities using the volume rendering integral:\n",
    "$$C(\\mathbf{r}) = \\int_{t_n}^{t_f} T(t) \\sigma(t) \\mathbf{c}(t) \\, dt$$\n",
    "where $T(t) = \\exp\\left(-\\int_{t_n}^{t} \\sigma(s) \\, ds\\right)$ is the transmittance (i.e., how much light reaches the point without being absorbed)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2507d794",
   "metadata": {},
   "source": [
    "#### More on Time-contrastive Network:\n",
    "\n",
    "A Time-Contrastive Network (TCN) is a self-supervised learning approach introduced to learn useful representations from temporal signals (like videos or audio) without requiring labels. The main idea is to use time as a source of supervision — hence the name time-contrastive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a7e6e6",
   "metadata": {},
   "source": [
    "#### NeRF + AutoEncoding + time-contrastive loss + dynamic model\n",
    "\n",
    "here, we propose embedding neural radiance fields (NeRF)\n",
    "into an autoencoder framework, enabling tractable inference of the\n",
    "3D-structure-aware scene state for dynamic environments.  By also\n",
    "enforcing a time contrastive loss on the estimated states, we ensure\n",
    "that the learned state representations are viewpoint-invariant. We\n",
    "then train a dynamics model that predicts the evolution of the state\n",
    "space conditioned on theinputaction, enablingcontrol inthelearned\n",
    "state space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d20caa8",
   "metadata": {},
   "source": [
    "Auto encoding framework:\n",
    "\n",
    "$$x \\xrightarrow{\\text{Encoder}} z \\xrightarrow{\\text{Decoder}} \\hat{x} \\approx x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfc1ba7",
   "metadata": {},
   "source": [
    "the approach can be summerized as follows:\n",
    "1. We extend an autoencoding framework with a neural radiance field rendering module and time contrastive learning, enabling the learning of 3D-aware scene representations for dynamics modelling and control purely from visual observations.\n",
    "2. By incorporating the autodecoder mechanism at test time, our framework can modify the learned representation and accomplish control tasks with the goal specified from camera viewpoints outside the training distribution. \n",
    "3. We are the first to augment neural radiance fields with a time-invariant dynamics\n",
    "model, supporting future prediction and novel view synthesis across a wide range of environments with diﬀerent object types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b420c7ea",
   "metadata": {},
   "source": [
    "#### 3D neural fields for dynamic modelling:\n",
    "1. an\n",
    "encoder that maps the input images in to a latent state representation,\n",
    "2. a decoder that generates an observation image under a certain\n",
    "viewpoint based on the state representation, and\n",
    "3. a dynamics\n",
    "model that predicts the future state representations based on the\n",
    "current state and the input action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a43a05",
   "metadata": {},
   "source": [
    "<img src=./images/nerf-autoencoder-time-contrastive-loss.png width=650>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9812cf18",
   "metadata": {},
   "source": [
    "#### neural radiance field for Dynamic scenes:\n",
    "\n",
    "To enable $f_{NeRF}$ to model dynamic scenes, we\n",
    "learn an encoding function $f_{enc}$ that maps the visual observations\n",
    "to a feature representation $z$ for each time step and learn the volumetric radiance field decoding function based on $z$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d1258e",
   "metadata": {},
   "source": [
    "Time contrastive loss in this architecture:\n",
    "\n",
    "**Time contrastive learning:** To enable the image encoder to be viewpoint invariant, we regularise the feature representation of each image $v^i_t$ using **multi-view time contrastive loss (TCN)**. The TCN loss encourages the features of images from different viewpoints at the same time step to be similar, while repulsing features of images from different time steps to be dissimilar. More specifically, given a time step $t$, we randomly select one image $I^i_t$ as the anchor and extract its image feature $v^i_t$ using the image encoder. Then we randomly select one positive image from the same time step but different camera viewpoint $I^j_t$ and one negative image from a different time step but the same viewpoint $I^i_t$. We use the same image encoder to extract their image features $v^j_t$ and $v^i_t$. we minimise the following time contrastive loss:\n",
    "\n",
    "$$L_{tc} = \\max(\\|v^i_t - v^j_t\\|_2^2 - \\|v^i_t - v^i_t\\|_2^2 + \\alpha, 0),$$\n",
    "\n",
    "where $\\alpha$ is a hyperparameter denoting the margin between the positive and negative pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0774c7",
   "metadata": {},
   "source": [
    "#### Online plannig:\n",
    "\n",
    "find the action sequence that minimises the distance between the predicted future representation and the goal representation at time $T$. given a sequence of actions, our model\n",
    "can iteratively predict a sequence of latent state representations.\n",
    "The latent-space dynamics model can then be used for downstream\n",
    "**closed-loop control** tasks via online planning with model-predictive control (**MPC**)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3133af",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
