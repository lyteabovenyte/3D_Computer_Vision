{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d758e673",
   "metadata": {},
   "source": [
    "### You Only Need One Thing One Click: \n",
    "#### Self-Training for Weakly Supervised 3D Scene Understanding\n",
    "\n",
    "source [paper](https://arxiv.org/pdf/2303.14727)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0069cf26",
   "metadata": {},
   "source": [
    "full story short: **just one point need to be annotated for any object in the scene.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40f9e14",
   "metadata": {},
   "source": [
    "$?$ can we achieve a performance comparable to a fully supervised baseline given the extremely sparse annotations?\n",
    "\n",
    "To meet such a challenge, we propose to design a self-training\n",
    "approach with a label-propagation mechanism for weakly supervised\n",
    "semantic segmentation. On the one hand, with the prediction result\n",
    "of the model, the pseudo labels can be expanded to unknown regions\n",
    "through our graph propagation module. On the other hand, with\n",
    "richer and higher quality labels being generated, the model performance can be further improved. Thus, we conduct the label propagation and network training iteratively, forming a closed loop to boost\n",
    "the performance of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cb72a4",
   "metadata": {},
   "source": [
    "Core Contributions\n",
    "\n",
    "This research introduces a self-training framework for 3D scene understanding, particularly focusing on semantic and instance segmentation tasks, using extremely sparse annotations. The key innovations include: ￼\n",
    "1.\tMinimal Annotation Strategy: The approach requires annotating only a single point per object, significantly reducing the manual labeling effort compared to traditional methods. ￼\n",
    "2.\tIterative Self-Training with Label Propagation: The model employs an iterative training process where it alternates between training on the current set of labels and propagating labels to unlabeled data points. This propagation is facilitated by a graph-based module that considers the spatial and feature similarities between points. ￼\n",
    "3.\tCategory Prototype Generation: A relation network is used to generate per-category prototypes, which help in enhancing the quality of pseudo-labels during the training iterations. ￼\n",
    "4.\tExtension to Instance Segmentation: Beyond semantic segmentation, the framework is adapted for instance segmentation tasks by incorporating a point-clustering strategy, allowing the model to distinguish between different object instances within the same category. ￼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bc855f",
   "metadata": {},
   "source": [
    "core innovation: the **Category-Aware Label Propagation**\n",
    "- To spread labels from the few clicked points to nearby points, the model uses:\n",
    "  - Graph-based label propagation, considering spatial and feature similarity.\n",
    "  - Category prototypes: these are learned representations (or “embeddings”) of each object category, guiding the label spreading process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ba09b7",
   "metadata": {},
   "source": [
    "**super-voxel over-segmentation**:\n",
    "\n",
    "A super-voxel is the 3D equivalent of a super-pixel in image processing.\n",
    "- In 2D, super-pixels group adjacent pixels with similar color or texture into coherent regions.\n",
    "- In 3D, super-voxels do the same for points in a point cloud: they group geometrically and visually similar 3D points into compact regions.\n",
    "\n",
    "These super-voxels are:\n",
    "- Geometrically homogeneous: points have similar surface normals or curvature\n",
    "- Compact in 3D space\n",
    "- Non-overlapping and complete:\n",
    "$$\\bigcup_j v_j = X,\\quad v_j \\cap v_{j{\\prime}} = \\emptyset \\text{ for } j \\ne j{\\prime}$$\n",
    "\n",
    "So, each point $p_i \\in X$ belongs to exactly one super-voxel $v_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb929ae",
   "metadata": {},
   "source": [
    "Overview architecture:\n",
    "\n",
    "<img src=./images/One-thing_One-click.png width=650>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a92e5a",
   "metadata": {},
   "source": [
    "the most important part of this diagram can be summerized as below:\n",
    "- 3D semantic segmentation network $\\Theta$ (3D U-Net)\n",
    "- relation Network $\\text{R}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a45c77",
   "metadata": {},
   "source": [
    "3D U-Net architecture as $\\Theta$.\n",
    "\n",
    "$$L_s = -\\frac{1}{N} \\sum_{i=1}^{N} \\log P(y_i, \\bar{c} | p_i, c_i, \\Theta).$$\n",
    "\n",
    "where $\\bar{c}$ is the ground-truth category for the point $p_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46503d77",
   "metadata": {},
   "source": [
    "**Pseudo Label Propagation** part: (graph-model based or transformer based)\n",
    "\n",
    "**3D U-Net for Semantic Label Prediction** (Blue Path)\n",
    "\n",
    "Purpose:\n",
    "- Learns semantic features at the point level from the raw 3D input (geometry, color, etc.).\n",
    "- Produces point-wise semantic predictions (e.g., “chair”, “table”, etc.).\n",
    "\n",
    "Architecture:\n",
    "- This is a 3D adaptation of the well-known U-Net architecture:\n",
    "- Encoder: progressively downsamples spatial resolution while increasing feature dimensionality.\n",
    "- Decoder: upsamples and combines features from the encoder (via skip connections) to preserve fine details.\n",
    "- Operates on 3D volumes or point-wise features (depending on implementation, often voxelized input is used).\n",
    "\n",
    "Why U-Net?\n",
    "- U-Net preserves both local (fine-grained) and global (contextual) features, which is crucial for semantic segmentation.\n",
    "- It gives strong point-level features that will later be aggregated over super-voxels.\n",
    "\n",
    "\n",
    "**Relation Net for Super-Voxel Similarity Learning** (Green Path)\n",
    "\n",
    "Purpose:\n",
    "- Learns a feature embedding per super-voxel, capturing semantic and relational structure.\n",
    "- Used to compute pairwise affinities between super-voxels, which later guides label propagation.\n",
    "\n",
    "Architecture & Inputs:\n",
    "- Takes input from:\n",
    "    - Super-voxel partition of the point cloud\n",
    "\t- Possibly pooled U-Net features from inside the super-voxel\n",
    "\t- Geometric information (e.g., centroid, normal, curvature)\n",
    "\t- Learns a super-voxel-level representation using:\n",
    "\t- MLPs or small CNNs\n",
    "\t- Contrastive or similarity-based loss (depending on paper details)\n",
    "\n",
    "What’s “Relation” About It?\n",
    "- It learns to encode how similar or different two super-voxels are.\n",
    "- This relation is used later in graph-based label propagation, where:\n",
    "- Unary potentials come from the 3D U-Net\n",
    "- Pairwise terms (affinities) are computed via Relation Net\n",
    "- Combined via graph inference or a Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58daaec",
   "metadata": {},
   "source": [
    "more on Relation Net($\\mathcal{R}$):\n",
    "\n",
    "The relation network $\\mathcal{R}$ shares the same backbone architecture as\n",
    "the 3D U-Net $\\Theta$ except for removing the last category-wise prediction\n",
    "layer. It aims to predict a category-related embedding $f_j$ for each\n",
    "super-voxel $v_j$ as the similarity measurement. $f_j$ is the per super-\n",
    "voxel pooled feature in $\\mathcal{R}$. In other words,the relation network groups\n",
    "the embeddings of the same category, while pushing those of diﬀerent\n",
    "categories apart. To this end, we propose to learn a prototypical\n",
    "embedding for each category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbead90",
   "metadata": {},
   "source": [
    "then we use this category-related embedding $f_j$ generated by $\\mathcal{R}$ as the Query and the $k_c$ in the memory bank as the key, the two modules are optimized simultaneously with **contrastive learning**:\n",
    "\n",
    "$$L_c = \\frac{1}{M} \\sum_{j} \\left( -\\log \\frac{f_j \\cdot k \\bar{c}/\\tau}{\\sum_{c} f_j \\cdot k c/\\tau} \\right)$$\n",
    "\n",
    "where $\\tau$ is the temprature hyperparameter, the contrastive learning is equivalent to c-way softmax classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670ce4af",
   "metadata": {},
   "source": [
    "Our relation net complements with 3D U-Net. It measures the\n",
    "relations between super-voxels using diﬀerent training strategies and\n",
    "losses, while 3D U-Net aims to project the inputs into the latent feature space for category assignment. The prediction of relation\n",
    "network is further combined with the prediction of 3D U-Net by\n",
    "multiplying the predicted possibilities of each category to boost the\n",
    "performance. In addition, the relation net oﬀers a stronger measurement of the pairwise term in CRF vs. handcrafted features like colours and also complements with the 3D U-Net features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2ff967",
   "metadata": {},
   "source": [
    "**the training step in Relation Network**:\n",
    "\n",
    "- The Relation Network outputs embedding f_j for every voxel.\n",
    "- For each voxel j, we compute similarity with all prototypes $p_k$:\n",
    "$\\text{sim}(f_j, p_k)$\n",
    "- Use softmax over similarities to treat this as a probability distribution.\n",
    "- Use the U-Net’s UnaryToken to guide the correct prototype:\n",
    "$$\\text{Loss}_{\\text{relation}} = \\text{KL-Divergence}(\\text{UnaryToken}_j \\parallel \\text{softmax}(\\text{sim}(f_j, p_k)))$$\n",
    "\n",
    "This is called **Unary-Guided Contrastive Learning**.\n",
    "\n",
    "In short:\n",
    "- U-Net learns semantic segmentation from sparse points.\n",
    "- Relation Network learns object-level grouping using the U-Net’s predictions and contrastive embedding losses.\n",
    "\n",
    "\n",
    "The 3D U-Net and Relation Network work in tandem within a self-training framework. The U-Net first generates pseudo-labels for unlabeled points based on the sparse annotations. The Relation Network then uses these pseudo-labels to learn relational patterns, refining the predictions. The refined predictions are fed back into the U-Net as pseudo-labels for the next iteration, creating a feedback loop that improves both networks’ performance over time. This synergy allows the model to generalize better with minimal supervision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634d5f30",
   "metadata": {},
   "source": [
    "**Transformer-based label propagation**:\n",
    "\n",
    "Unlike the graph model-based approach that learns the aﬃnity\n",
    "among super-voxels, where the size of the aﬃnity matrix $M \\times M$\n",
    "grows quadratically relative to the number of super-voxels $M$, the\n",
    "transformer-based label propagation aims to learn the correlation\n",
    "between a super-voxel $v_j$ and a category prototype $k_c$. Therefore,\n",
    "the size of the attention map $M \\times c$ grows proportionally to $M$,\n",
    "significantly improving eﬃciency in terms of memory and inference\n",
    "time. Additionally, transformer-based label propagation can be optimised end-to-end, further improving the performance of 3D semantic segmentation.\n",
    "\n",
    "$$\\hat{f}_j = \\Sigma_{c} \\text{softmax} \\left( \\frac{Q(F_j) K(k_c)}{\\sqrt{d_l}} \\right) V(k_c)$$\n",
    "\n",
    "where $Q$, $K$, and $V$ represent MLP layers, while $F_j$ represents\n",
    "the feature vector of the 3D U-Net. The transformer then aggregates the category prototype kc based on the similarity between $F_j$\n",
    "and $k_c$. The resulting output feature\n",
    "$\\hat{f}_j$ is then concatenated with $F_j$\n",
    "to make the final prediction for the semantic category.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f483b962",
   "metadata": {},
   "source": [
    "#### self-training mechanism\n",
    "\n",
    "With the label propagation(Using U-Net $F_j$ and Relation-Net), there is a self-training approach\n",
    "to update networks $\\Theta$ and $\\mathcal{R}$ and also the pseudo labels $Y$ iteratively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6570ad7",
   "metadata": {},
   "source": [
    "first let's research more about **CRF(conditional Random Field)**:\n",
    "\n",
    "A Conditional Random Field (CRF) is a type of probabilistic graphical model used for structured prediction. Unlike models that make independent predictions (e.g., logistic regression), CRFs jointly model the distribution of all outputs $Y$ given inputs $X$, while capturing dependencies between the outputs.\n",
    "\n",
    "Formal Definition:\n",
    "$$P(Y | X) = \\frac{1}{Z(X)} \\exp(-E(Y | X))$$\n",
    "\n",
    "CRF model label dependencies, not just individual label likelihook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac016336",
   "metadata": {},
   "source": [
    "self-training loop:\n",
    "\n",
    "With $\\Theta$ and $\\mathcal{R}$ fixed, the label propagation is conducted to minimise the energy function. Then, the predictions\n",
    "with high confidence are taken as the updated pseudo labels for\n",
    "training the two networks in the following iteration. The confidence of super-voxel $v_j$ , denoted as $C_j$ , is the average of the minus\n",
    "log probability of all $n_j$ points in $v_j$ after the label propagation:\n",
    "\n",
    "$$C_j = \\frac{1}{n_j} \\sum_i^{n_j} \\log P(y_i \\mid \\mathbf{p}_i, \\mathbf{V}, \\mathbf{\\Theta}, \\mathcal{R}, \\mathbf{G}), \\quad \\text{where} \\quad \\mathbf{p}_i \\in \\mathbf{v}_j$$\n",
    "\n",
    "where $G$ denotes the graph propagation. With pseudo labels $Y$, $\\Theta$ and $\\mathcal{R}$ are optimised, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434aee7a",
   "metadata": {},
   "source": [
    "**3D instance segmentation**:\n",
    "\n",
    "[**PointGroup**](https://arxiv.org/abs/2004.01658) approach on 3D instance segmentation, using dual-set clustering and a MLP head to compute the offset of each point cloud to the centroid of each cluster, after that using Non-Maximum Suppression (NMS) and reducing mIoU(Mean Intersection over Union)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
