{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da5198da",
   "metadata": {},
   "source": [
    "### Masked Autoencoders for 3D point cloud self-supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c292065",
   "metadata": {},
   "source": [
    "##### high-level Notes on the concept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec7cbf4",
   "metadata": {},
   "source": [
    "It proposes a BERT-style pre-training strategy by\n",
    "masking input tokens of the point cloud and then adopts a transformer architecture to predict discrete tokens of the masked tokens.\n",
    "However, this method is relatively sophisticated as it is required to\n",
    "train a DGCNN-based discrete Variational AutoEncoder(dVAE)\n",
    "before pre-training and relies heavily on contrastive learning as well\n",
    "as data augmentation during pre-training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08bf2fe",
   "metadata": {},
   "source": [
    "Moreover, the masked\n",
    "tokens from their inputs are processed from the input of transformer during pre-training, leading to early leakage of location information\n",
    "and high consumption of computing resources. Diﬀerent from their\n",
    "method, and more importantly, to introduce masked autoencoding\n",
    "to the point cloud, we aim to design a neat and eﬃcient scheme of\n",
    "masked autoencoders. To this end, we first analyse the main chal-\n",
    "lenges of introducing masked autoencoding for point cloud from the\n",
    "following aspects:\n",
    "\n",
    "1. The first is the lack of a unified transformer architecture. Com-\n",
    "pared to transformer in NLP and Vision Transformer (ViT)\n",
    "in computervision, transformerarchitectures forpointcloud are\n",
    "lessstudiedandrelatively diverse,mainlybecausesmalldatasets\n",
    "cannot meet the large data demand of transformer. Diﬀerent\n",
    "from previous methods that use dedicated transformer or adopt\n",
    "extra non-transformer models to assist (such as **Point-BERT**\n",
    "uses an extra **DGCNN**), we aim to build our autoencoder’s\n",
    "backbone entirely based on standard transformer, which can\n",
    "serve as a potential unified architecture for point cloud. This\n",
    "also enables further development for point cloud to join general\n",
    "multi-modality frameworks, such as **Data2vec**.\n",
    "\n",
    "2. Positional embeddings for mask tokens lead to **leakage of location** information. In masked autoencoders, each masked part\n",
    "is replaced by a share-weighted learnable mask token. All the\n",
    "mask tokens need to be provided with their location information in input data by positional embeddings. Then after\n",
    "processing by autoencoders, each mask token is used to reconstruct the corresponding masked part. Providing location information is not an issue for languages and images because they\n",
    "do not contain location information. While point cloud naturally has location information in the data, leakage of location\n",
    "information to mask tokens makes the reconstruction task less\n",
    "challenging, which is harmful for autoencoders learning latent\n",
    "features. We address this issue by shifting mask tokens from the\n",
    "input of the autoencoder’s encoder to the input of the autoencoder’s decoder. This delays the leakage of location information and enables the encoder to focus on learning features from\n",
    "unmasked parts.\n",
    "\n",
    "3. Point cloud carries information in a diﬀerent density compared to languages and images. Languages contain high-density information, while images contain heavy redundant informa-\n",
    "tion.12 In the point cloud, information density distribution is\n",
    "relatively uneven. The points that make up key local features\n",
    "(e.g., sharp corners and edges) contain a much higher den-\n",
    "sity of information than the points that make up less impor-\n",
    "tant local features (e.g., flat surfaces). In other words, if being\n",
    "masked, the points that contain high-density information are\n",
    "more diﬃcult to be recovered in the reconstruction task. This\n",
    "can be directly observed in reconstruction examples, as shown\n",
    "in Figure 2. Taking the last row of Figure 2 for illustration,\n",
    "the masked desk surface (left) can be easily recovered, while\n",
    "the reconstruction of the masked motorcycle’s wheel (right) is\n",
    "much worse. Although the point cloud contains uneven density\n",
    "of information, we find that random masking at a high ratio\n",
    "(60–80%) works well, which is surprisingly the same as images.\n",
    "This indicates the point cloud is similar to images instead of\n",
    "languages, in terms of information density."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ca8141",
   "metadata": {},
   "source": [
    "<img src=./images/Point_MAE.png width=650 style=\"display:block; margin:auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b9bea7",
   "metadata": {},
   "source": [
    "**FPS**:\n",
    "The point cloud is processed using FPS (Farthest Point Sampling), a technique to select a subset of points (called centers) that are well-distributed across the point cloud. This reduces the number of points while preserving the overall structure\n",
    "\n",
    "**KNN**: Using KNN (K-Nearest Neighbors), the point cloud is divided into point patches. Each patch is a group of points centered around one of the FPS-selected centers. For example, if a center point is selected, KNN finds its K nearest neighbors in the point cloud, forming a small local patch of points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aa2e03",
   "metadata": {},
   "source": [
    "After the decoder, a **one-layer prediction head** (a simple neural network layer) is used to map the decoder’s output to the desired format.\n",
    "The output of the prediction head is a prediction (Pred) of the masked point patches. Specifically, it predicts the coordinates or features of the points in the masked patches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf54c58e",
   "metadata": {},
   "source": [
    "The loss function used is the **Chamfer Distance (CD)** with an ${l}_2$ norm:\n",
    "Chamfer Distance measures the distance between two point sets (predicted and ground truth) by finding the nearest point correspondences.\n",
    "The $l_2$ norm ensures the predicted points are as close as possible to the ground truth points in terms of Euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be0577c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on Point-MAE: Masked Autoencoders for Point Clouds paper\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pytorch3d.loss import chamfer_distance\n",
    "from pytorch3d.ops import sample_farthest_points, knn_points\n",
    "\n",
    "# Hyperparameters\n",
    "NUM_POINTS = 2048  # Total points in the point cloud\n",
    "NUM_PATCHES = 128  # Number of patches\n",
    "POINTS_PER_PATCH = 16  # Points per patch (NUM_POINTS / NUM_PATCHES)\n",
    "MASK_RATIO = 0.8  # Mask 80% of patches\n",
    "EMBED_DIM = 64  # Embedding dimension for tokens\n",
    "NUM_HEADS = 4  # Number of attention heads in Transformer\n",
    "NUM_LAYERS = 2  # Number of Transformer layers\n",
    "\n",
    "class PointMAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PointMAE, self).__init__()\n",
    "        \n",
    "        # Token embedding layer for point patches\n",
    "        self.patch_embed = nn.Linear(POINTS_PER_PATCH * 3, EMBED_DIM)\n",
    "        \n",
    "        # Positional encoding for patch centers\n",
    "        self.pos_embed = nn.Linear(3, EMBED_DIM)\n",
    "        \n",
    "        # Mask token (learnable parameter)\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, EMBED_DIM))\n",
    "        \n",
    "        # Encoder: Transformer for visible tokens\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=EMBED_DIM, nhead=NUM_HEADS)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=NUM_LAYERS)\n",
    "        \n",
    "        # Decoder: Lightweight Transformer\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=EMBED_DIM, nhead=NUM_HEADS)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=1)\n",
    "        \n",
    "        # Prediction head to reconstruct point patches\n",
    "        self.pred_head = nn.Linear(EMBED_DIM, POINTS_PER_PATCH * 3)\n",
    "\n",
    "    def patchify(self, point_cloud):\n",
    "        \"\"\"\n",
    "        Divide point cloud into patches using FPS and KNN.\n",
    "        Input: point_cloud (batch_size, num_points, 3)\n",
    "        Output: patches (batch_size, num_patches, points_per_patch, 3), centers (batch_size, num_patches, 3)\n",
    "        \"\"\"\n",
    "        batch_size = point_cloud.shape[0]\n",
    "        \n",
    "        # FPS to select patch centers\n",
    "        centers, _ = sample_farthest_points(point_cloud, K=NUM_PATCHES)\n",
    "        \n",
    "        # KNN to group points into patches\n",
    "        _, idx, _ = knn_points(centers, point_cloud, K=POINTS_PER_PATCH)\n",
    "        \n",
    "        # Gather points for each patch\n",
    "        patches = torch.gather(\n",
    "            point_cloud.unsqueeze(1).expand(-1, NUM_PATCHES, -1, -1),\n",
    "            2,\n",
    "            idx.unsqueeze(-1).expand(-1, -1, -1, 3)\n",
    "        )  # (batch_size, num_patches, points_per_patch, 3)\n",
    "        \n",
    "        return patches, centers\n",
    "\n",
    "    def mask_patches(self, patches):\n",
    "        \"\"\"\n",
    "        Randomly mask patches.\n",
    "        Input: patches (batch_size, num_patches, points_per_patch, 3)\n",
    "        Output: visible_patches, mask (binary mask indicating masked patches)\n",
    "        \"\"\"\n",
    "        batch_size = patches.shape[0]\n",
    "        num_patches = patches.shape[1]\n",
    "        num_masked = int(num_patches * MASK_RATIO)\n",
    "        \n",
    "        # Randomly shuffle patch indices\n",
    "        idx = torch.rand(batch_size, num_patches, device=patches.device).argsort(dim=1)\n",
    "        mask = torch.ones(batch_size, num_patches, device=patches.device, dtype=torch.bool)\n",
    "        mask.scatter_(1, idx[:, :num_masked], 0)  # Set masked patches to 0\n",
    "        \n",
    "        return mask\n",
    "\n",
    "    def forward(self, point_cloud):\n",
    "        \"\"\"\n",
    "        Forward pass of Point-MAE.\n",
    "        Input: point_cloud (batch_size, num_points, 3)\n",
    "        Output: reconstructed patches, loss\n",
    "        \"\"\"\n",
    "        batch_size = point_cloud.shape[0]\n",
    "        \n",
    "        # Step 1: Patchify the point cloud\n",
    "        patches, centers = self.patchify(point_cloud)  # (batch_size, num_patches, points_per_patch, 3)\n",
    "        \n",
    "        # Step 2: Mask patches\n",
    "        mask = self.mask_patches(patches)  # (batch_size, num_patches)\n",
    "        \n",
    "        # Step 3: Embed patches into tokens\n",
    "        patches_flat = patches.view(batch_size, NUM_PATCHES, -1)  # (batch_size, num_patches, points_per_patch * 3)\n",
    "        tokens = self.patch_embed(patches_flat)  # (batch_size, num_patches, embed_dim)\n",
    "        \n",
    "        # Add positional encoding based on patch centers\n",
    "        pos_enc = self.pos_embed(centers)  # (batch_size, num_patches, embed_dim)\n",
    "        tokens = tokens + pos_enc\n",
    "        \n",
    "        # Step 4: Separate visible tokens for encoder\n",
    "        visible_mask = mask.unsqueeze(-1).expand(-1, -1, EMBED_DIM)  # (batch_size, num_patches, embed_dim)\n",
    "        visible_tokens = tokens * visible_mask  # Zero out masked tokens\n",
    "        visible_tokens = visible_tokens[mask.bool()].view(batch_size, -1, EMBED_DIM)  # (batch_size, num_visible, embed_dim)\n",
    "        \n",
    "        # Step 5: Encoder processes visible tokens\n",
    "        encoded = self.encoder(visible_tokens.transpose(0, 1)).transpose(0, 1)  # (batch_size, num_visible, embed_dim)\n",
    "        \n",
    "        # Step 6: Prepare decoder input (reinsert mask tokens)\n",
    "        decoder_input = tokens.clone()\n",
    "        mask_idx = (mask == 0).unsqueeze(-1).expand(-1, -1, EMBED_DIM)\n",
    "        decoder_input[mask_idx] = self.mask_token.expand(batch_size, NUM_PATCHES, -1)[mask_idx]\n",
    "        \n",
    "        # Step 7: Decoder reconstructs masked patches\n",
    "        decoded = self.decoder(\n",
    "            tgt=decoder_input.transpose(0, 1),\n",
    "            memory=encoded.transpose(0, 1)\n",
    "        ).transpose(0, 1)  # (batch_size, num_patches, embed_dim)\n",
    "        \n",
    "        # Step 8: Prediction head reconstructs patches\n",
    "        recon_patches = self.pred_head(decoded)  # (batch_size, num_patches, points_per_patch * 3)\n",
    "        recon_patches = recon_patches.view(batch_size, NUM_PATCHES, POINTS_PER_PATCH, 3)\n",
    "        \n",
    "        # Step 9: Compute Chamfer Distance loss (only for masked patches)\n",
    "        masked_patches = patches[mask == 0]  # Ground truth masked patches\n",
    "        recon_masked = recon_patches[mask == 0]  # Reconstructed masked patches\n",
    "        if masked_patches.shape[0] > 0:  # Ensure there are masked patches\n",
    "            loss, _ = chamfer_distance(recon_masked, masked_patches)\n",
    "        else:\n",
    "            loss = torch.tensor(0.0, device=point_cloud.device)\n",
    "        \n",
    "        return recon_patches, loss\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Dummy point cloud (batch_size, num_points, 3)\n",
    "    point_cloud = torch.rand(2, NUM_POINTS, 3)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = PointMAE()\n",
    "    \n",
    "    # Forward pass\n",
    "    recon_patches, loss = model(point_cloud)\n",
    "    print(f\"Chamfer Distance Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68094dfd",
   "metadata": {},
   "source": [
    "so the whole concept of **Point_MAE**:\n",
    "\n",
    "Driven by the analysis, we propose a novel self-supervisedlearning\n",
    "framework for Point cloud by designing a neat and eﬃcient scheme\n",
    "of Masked AutoEncoders, termed as Point-MAE. this Point-MAE mainly consists of a point cloud masking\n",
    "and embedding module and an autoencoder. The input point cloud\n",
    "is divided into irregular point patches, which are randomly masked\n",
    "at a high ratio to reduce data redundancy. Then, the autoencoder\n",
    "learnshigh-level latentfeaturesfromunmaskedpointpatches, aiming\n",
    "to reconstruct masked point patches in coordinate space. Specifically,\n",
    "our autoencoder’s backbone is entirely built by standard transformer\n",
    "blocks and adopts an asymmetric encoder–decoder structure.12 The\n",
    "encoder only processes unmasked point patches. Then taking both\n",
    "encoded tokens and mask tokens as input, the lightweight decoder\n",
    "with a simple prediction head reconstructs masked point patches.\n",
    "Compared to processing mask tokens from the input of the encoder,\n",
    "shifting mask tokens to the lightweight decoder results in significant\n",
    "computational savings, more importantly, avoiding early leakage of\n",
    "location information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dedb82d",
   "metadata": {},
   "source": [
    "various SSL (self-supervised learning) strategy and pre-text task used for Point cloud data:\n",
    "\n",
    "SSL has also been widely studied for point cloud representation learning. Pre-text tasks are relatively diverse. Among them, DepthContrast sets an instance discrimination task for two augmented\n",
    "versions of an input point cloud. OcCo attempts to recover the\n",
    "original point cloud from the occluded point cloud in camera views.\n",
    "IAE adopts an autoencoder to reconstruct implicit features from\n",
    "augmented inputs. A recent work Point-BERT proposes a BERT-style pre-training strategy by masking input tokens and aims to predict discrete tokens of masked parts, with the assistance of dVAE.\n",
    "Diﬀerent from previous methods,we attempt to design a neat scheme\n",
    "for point cloud self-supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bf53e8",
   "metadata": {},
   "source": [
    "#### Important note on point-clouds patching and embedding:\n",
    "\n",
    "Unlike images in computer vision that can be naturally divided into\n",
    "regular patches, point cloud consists of unorderedpoints in 3D space.\n",
    "Based onits property, weprocesstheinputpointcloud throughthree\n",
    "stages: **point patches generation, masking, and embedding**.\n",
    "\n",
    "$$\n",
    "CT = FPS(X^i), \\quad CT \\in \\mathbb{R}^{n \\times 3} \\\\\n",
    "\n",
    "P = KNN(X^i, CT), \\quad P \\in \\mathbb{R}^{n \\times k \\times 3}.\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecc0bbf",
   "metadata": {},
   "source": [
    "For the embedding of each masked point patch, we replace it with\n",
    "a share-weighted learnable mask token. We denote the full set of\n",
    "mask tokens as $T_m \\in R^{mn \\times C}$ , where $C$ is the embedding dimension.\n",
    "For the unmasked (visible) point patches, a naive idea is to flatten\n",
    "and embed them with a trainable linear projection, similar to ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acccc66",
   "metadata": {},
   "source": [
    "encoder-decoder structure:\n",
    "\n",
    "$$\n",
    "\n",
    "T_e = \\text{Encoder}(T_v), \\quad T_e \\in \\mathbb{R}^{(1-m)n \\times C} \\\\\n",
    "\n",
    "H_m = \\text{Decoder}(\\text{concat}(T_e, T_m)), \\quad H_m \\in \\mathbb{R}^{mn \\times C}.\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9179b427",
   "metadata": {},
   "source": [
    "prediction head --> predicted masked point patches $P_{pre}$:\n",
    "\n",
    "$$\n",
    "P_{\\text{pre}} = \\text{Reshape}(\\text{FC}(H_m)), \\quad P_{\\text{pre}} \\in \\mathbb{R}^{mn \\times k \\times\n",
    "3}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bddb10",
   "metadata": {},
   "source": [
    "$H_m$ is the output from the decoder. which is given $T_e$ from encoder concatenated by $T_m$ which is masked tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4227e9",
   "metadata": {},
   "source": [
    "Our reconstruction target is to recover the coordinates of the points\n",
    "in every masked point patch. Given the predicted point patches\n",
    "$P_{pre}$ and ground truth $P_{gt}$, we compute the reconstruction loss by\n",
    "**l2 Chamfer Distance**:\n",
    "\n",
    "$$L = \\frac{1}{|P_{\\text{pre}}|} \\sum_{a \\in P_{\\text{pre}}} \\min_{b \\in P_{\\text{gt}}} \\| a - b \\|_2^2 + \\frac{1}{|P_{\\text{gt}}|} \\sum_{b \\in P_{\\text{gt}}} \\min_{a \\in P_{\\text{pre}}} \\| a - b \\|_2^2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c5da15",
   "metadata": {},
   "source": [
    "more on l2 chamfer distance:\n",
    "\n",
    "Chamfer Distance (CD) is a metric used to measure the similarity between two point sets. It’s widely used in 3D shape reconstruction and generative models.\n",
    "\n",
    "It’s a bidirectional matching between two point sets:\n",
    "- The first term says: “For each predicted point, find the closest ground-truth point, and penalize the squared L2 distance.”\n",
    "- The second term says: “For each ground-truth point, find the closest predicted point, and penalize the squared L2 distance.”\n",
    "\n",
    "\n",
    "This ensures that:\n",
    "1.\tEvery predicted point is close to some real point (**precision**).\n",
    "2.\tEvery real point is reconstructed by some predicted point (**recall**)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd59787",
   "metadata": {},
   "source": [
    "experimentation:\n",
    "\n",
    "In our Point-MAE, for diﬀerent resolutions of the input point\n",
    "cloud, we divide them into diﬀerent numbers of patches with a linear\n",
    "scaling. A typical input with p = 1024 points is divided into n =\n",
    "64 point patches. For the KNN algorithm, we set k = 32 to keep\n",
    "the number of points in each patch constant. In the autoencoder’s\n",
    "backbone, the encoder has 12 transformer blocks, while the decoder\n",
    "has 4 transformer blocks. Each transformer block has 384 hidden\n",
    "dimensions and 6 heads. MLP ratio in transformer blocks is set to 4.\n",
    "Note in downstream tasks, the decoder is discarded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cd062d",
   "metadata": {},
   "source": [
    "### Joint_MAE (leveraging 2D images for point cloud data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31faa55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Vision Transformer for 2D images (simplified)\n",
    "class ViT2D(nn.Module):\n",
    "    def __init__(self, patch_size=16, dim=256, depth=6, heads=8):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.dim = dim\n",
    "        self.patch_embed = nn.Conv2d(3, dim, patch_size, stride=patch_size)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, 196, dim))\n",
    "        self.transformer = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(dim, heads) for _ in range(depth)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # x: (B, 3, H, W), e.g., (B, 3, 224, 224)\n",
    "        x = self.patch_embed(x).flatten(2).transpose(1, 2)  # (B, num_patches, dim)\n",
    "        x = x + self.pos_embed\n",
    "        if mask is not None:\n",
    "            x = x * mask.unsqueeze(-1)  # Apply mask\n",
    "        for layer in self.transformer:\n",
    "            x = layer(x)\n",
    "        return x  # (B, num_patches, dim)\n",
    "\n",
    "# Point Cloud Transformer for 3D (simplified)\n",
    "class PointTransformer3D(nn.Module):\n",
    "    def __init__(self, num_points=1024, dim=256, depth=6, heads=8):\n",
    "        super().__init__()\n",
    "        self.num_points = num_points\n",
    "        self.dim = dim\n",
    "        self.point_embed = nn.Linear(3, dim)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, num_points, dim))\n",
    "        self.transformer = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(dim, heads) for _ in range(depth)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # x: (B, num_points, 3)\n",
    "        x = self.point_embed(x)  # (B, num_points, dim)\n",
    "        x = x + self.pos_embed\n",
    "        if mask is not None:\n",
    "            x = x * mask.unsqueeze(-1)\n",
    "        for layer in self.transformer:\n",
    "            x = layer(x)\n",
    "        return x  # (B, num_points, dim)\n",
    "\n",
    "# Local-Aligned Attention for cross-modal interaction\n",
    "class LocalAlignedAttention(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(dim, dim)\n",
    "        self.key = nn.Linear(dim, dim)\n",
    "        self.value = nn.Linear(dim, dim)\n",
    "        self.scale = dim ** -0.5\n",
    "    \n",
    "    def forward(self, x_2d, x_3d):\n",
    "        # x_2d: (B, num_patches, dim), x_3d: (B, num_points, dim)\n",
    "        q = self.query(x_2d)  # (B, num_patches, dim)\n",
    "        k = self.key(x_3d)    # (B, num_points, dim)\n",
    "        v = self.value(x_3d)  # (B, num_points, dim)\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        out = attn @ v  # (B, num_patches, dim)\n",
    "        return out\n",
    "\n",
    "# Joint-MAE Model\n",
    "class JointMAE(nn.Module):\n",
    "    def __init__(self, dim=256, depth=6, heads=8):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "        # 2D and 3D Encoders\n",
    "        self.encoder_2d = ViT2D(dim=dim, depth=depth, heads=heads)\n",
    "        self.encoder_3d = PointTransformer3D(dim=dim, depth=depth, heads=heads)\n",
    "        \n",
    "        # Joint Encoder\n",
    "        self.joint_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(dim, heads), num_layers=depth\n",
    "        )\n",
    "        \n",
    "        # Modal-Shared Decoder\n",
    "        self.shared_decoder = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, dim)\n",
    "        )\n",
    "        \n",
    "        # Modal-Specific Decoders\n",
    "        self.decoder_2d = nn.Linear(dim, dim)\n",
    "        self.decoder_3d = nn.Linear(dim, dim)\n",
    "        \n",
    "        # Local-Aligned Attention\n",
    "        self.local_attn = LocalAlignedAttention(dim)\n",
    "    \n",
    "    def forward(self, img, points, mask_2d, mask_3d):\n",
    "        # img: (B, 3, H, W), points: (B, num_points, 3)\n",
    "        # mask_2d: (B, num_patches), mask_3d: (B, num_points)\n",
    "        \n",
    "        # Encode unmasked patches/points\n",
    "        feat_2d = self.encoder_2d(img, mask_2d)  # (B, num_patches, dim)\n",
    "        feat_3d = self.encoder_3d(points, mask_3d)  # (B, num_points, dim)\n",
    "        \n",
    "        # Cross-modal attention\n",
    "        feat_2d = feat_2d + self.local_attn(feat_2d, feat_3d)\n",
    "        \n",
    "        # Concatenate and joint encode\n",
    "        feat = torch.cat([feat_2d, feat_3d], dim=1)  # (B, num_patches+num_points, dim)\n",
    "        feat = self.joint_encoder(feat)\n",
    "        \n",
    "        # Split back\n",
    "        feat_2d = feat[:, :feat_2d.size(1)]\n",
    "        feat_3d = feat[:, feat_2d.size(1):]\n",
    "        \n",
    "        # Shared and specific decoding\n",
    "        shared_2d = self.shared_decoder(feat_2d)\n",
    "        shared_3d = self.shared_decoder(feat_3d)\n",
    "        out_2d = self.decoder_2d(shared_2d)  # (B, num_patches, dim)\n",
    "        out_3d = self.decoder_3d(shared_3d)  # (B, num_points, dim)\n",
    "        \n",
    "        return out_2d, out_3d\n",
    "\n",
    "# Loss Function with Cross-Reconstruction\n",
    "def joint_mae_loss(out_2d, out_3d, target_2d, target_3d, mask_2d, mask_3d):\n",
    "    # out_2d, out_3d: predicted embeddings\n",
    "    # target_2d, target_3d: ground truth embeddings\n",
    "    # mask_2d, mask_3d: binary masks for masked patches/points\n",
    "    \n",
    "    # Reconstruction loss (only for masked patches/points)\n",
    "    loss_2d = F.mse_loss(out_2d[mask_2d == 0], target_2d[mask_2d == 0])\n",
    "    loss_3d = F.mse_loss(out_3d[mask_3d == 0], target_3d[mask_3d == 0])\n",
    "    \n",
    "    # Cross-reconstruction loss\n",
    "    cross_loss_2d = F.mse_loss(out_2d[mask_2d == 0], target_3d[mask_2d == 0].mean(dim=1, keepdim=True))\n",
    "    cross_loss_3d = F.mse_loss(out_3d[mask_3d == 0], target_2d[mask_3d == 0].mean(dim=1, keepdim=True))\n",
    "    \n",
    "    return loss_2d + loss_3d + 0.5 * (cross_loss_2d + cross_loss_3d)\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    batch_size, num_points, img_size = 8, 1024, 224\n",
    "    img = torch.randn(batch_size, 3, img_size, img_size)\n",
    "    points = torch.randn(batch_size, num_points, 3)\n",
    "    mask_2d = torch.ones(batch_size, 196)  # Example: 224/16 = 14, 14*14 = 196 patches\n",
    "    mask_3d = torch.ones(batch_size, num_points)\n",
    "    mask_2d[:, :int(196*0.75)] = 0  # Mask 75% of 2D patches\n",
    "    mask_3d[:, :int(num_points*0.75)] = 0  # Mask 75% of 3D points\n",
    "    \n",
    "    model = JointMAE()\n",
    "    out_2d, out_3d = model(img, points, mask_2d, mask_3d)\n",
    "    \n",
    "    # Dummy targets (in practice, use encoder outputs on unmasked data)\n",
    "    target_2d = torch.randn_like(out_2d)\n",
    "    target_3d = torch.randn_like(out_3d)\n",
    "    \n",
    "    loss = joint_mae_loss(out_2d, out_3d, target_2d, target_3d, mask_2d, mask_3d)\n",
    "    print(f\"Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6f3eec",
   "metadata": {},
   "source": [
    "the improvement of Joint-MAE models, from it's previous models are the concept of **Local-Aligend Cross Attention** between 2D and 3D. (cross-model learning concept)\n",
    "- Select local neighborhoods of 2D patches and 3D points that likely correspond to the same region/object part.\n",
    "- Let 2D tokens attend to their semantically aligned 3D counterparts, and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d26c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LocalAlignedCrossAttention(nn.Module):\n",
    "    def __init__(self, dim, k_neighbors=16, heads=8):\n",
    "        super().__init__()\n",
    "        self.k = k_neighbors\n",
    "        self.heads = heads\n",
    "        self.scale = (dim // heads) ** -0.5\n",
    "        \n",
    "        self.to_q = nn.Linear(dim, dim)\n",
    "        self.to_kv = nn.Linear(dim, dim * 2)\n",
    "        self.to_out = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, img_tokens, pc_tokens, img_pos, pc_pos):\n",
    "        \"\"\"\n",
    "        img_tokens: (B, N_img, D)\n",
    "        pc_tokens:  (B, N_pc, D)\n",
    "        img_pos:    (B, N_img, 3) or 2D (x, y)\n",
    "        pc_pos:     (B, N_pc, 3)\n",
    "\n",
    "        Returns updated img_tokens.\n",
    "        \"\"\"\n",
    "        B, N_img, D = img_tokens.shape\n",
    "        N_pc = pc_tokens.size(1)\n",
    "        H = self.heads\n",
    "\n",
    "        # Project to Q (from image) and KV (from point cloud)\n",
    "        Q = self.to_q(img_tokens).view(B, N_img, H, D // H)\n",
    "        KV = self.to_kv(pc_tokens).view(B, N_pc, 2, H, D // H)\n",
    "        K, V = KV[:, :, 0], KV[:, :, 1]  # (B, N_pc, H, D_head)\n",
    "\n",
    "        # Step 1: Find KNN in pc_pos for each img_pos\n",
    "        # We assume positions are aligned across modalities\n",
    "        neighbors_idx = knn(pc_pos, img_pos, k=self.k)  # (B, N_img, k)\n",
    "\n",
    "        # Step 2: Gather K and V for each image token's local pc neighborhood\n",
    "        K_neighbors = batched_index_select(K, neighbors_idx)  # (B, N_img, k, H, D_head)\n",
    "        V_neighbors = batched_index_select(V, neighbors_idx)\n",
    "\n",
    "        # Step 3: Compute attention\n",
    "        Q = Q.unsqueeze(2)  # (B, N_img, 1, H, D_head)\n",
    "        attn = (Q * K_neighbors).sum(-1) * self.scale  # (B, N_img, k, H)\n",
    "        attn = F.softmax(attn, dim=2)\n",
    "\n",
    "        # Step 4: Apply attention\n",
    "        out = (attn.unsqueeze(-1) * V_neighbors).sum(2)  # (B, N_img, H, D_head)\n",
    "        out = out.reshape(B, N_img, D)\n",
    "\n",
    "        return self.to_out(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb17f8c8",
   "metadata": {},
   "source": [
    "in Point-MAE architecture, **transformer dimension** refers to the embedding dimension of tokens (i.e., the feature dimensionality of the inputs and outputs of the transformer layers). This dimension controls the size of the hidden representations in the encoder and decoder, and it is a key hyperparameter in transformer-based architectures.\n",
    "in the original paper, The authors describe a transformer architecture that operates on grouped point patches, where each patch is converted into a token (a feature vector). These tokens are then fed into a ViT-like transformer encoder.\n",
    "In their Point-MAE-Small configuration, they use D=384, and in Point-MAE-Base, D=768.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
