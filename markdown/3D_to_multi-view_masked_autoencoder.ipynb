{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f611b3b",
   "metadata": {},
   "source": [
    "### Point Cloud Self-supervised Learning via 3D to Multi-view Masked Autoencoder\n",
    "\n",
    "link to the [paper](https://arxiv.org/pdf/2311.10887v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3141ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import open3d as o3d\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278341c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example multi-view projection of 3D point clouds into multiple 2D images from different angles using open3d lib\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "\n",
    "# Load or create a point cloud\n",
    "pcd = o3d.io.read_point_cloud(\"model.ply\")  # or use your own point cloud\n",
    "\n",
    "# Define camera viewpoints\n",
    "camera_positions = [\n",
    "    [0, 0, 1],  # front\n",
    "    [1, 0, 0],  # side\n",
    "    [0, 1, 0],  # top\n",
    "]\n",
    "\n",
    "# Render projections\n",
    "images = []\n",
    "for pos in camera_positions:\n",
    "    vis = o3d.visualization.Visualizer()\n",
    "    vis.create_window(visible=False)\n",
    "    vis.add_geometry(pcd)\n",
    "\n",
    "    ctr = vis.get_view_control()\n",
    "    ctr.set_lookat(pcd.get_center())\n",
    "    ctr.set_front(pos)\n",
    "    ctr.set_up([0, 0, 1])\n",
    "    ctr.set_zoom(0.5)\n",
    "\n",
    "    vis.poll_events()\n",
    "    vis.update_renderer()\n",
    "    image = vis.capture_screen_float_buffer(False)\n",
    "    images.append(np.asarray(image))\n",
    "    vis.destroy_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cc9461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or if using PyTorch for implementing from scratch\n",
    "import torch\n",
    "# Suppose P is a (3, N) point cloud\n",
    "P_hom = torch.cat([P, torch.ones(P.shape[0], 1)], dim=1)  # Make it homogeneous (N, 4)\n",
    "\n",
    "# Camera projection matrix (extrinsics + intrinsics)\n",
    "projection_matrix = get_projection_matrix(view_angle)\n",
    "\n",
    "# Get 2D projections\n",
    "projected_2d = (projection_matrix @ P_hom.T).T\n",
    "projected_2d = projected_2d[:, :2] / projected_2d[:, 2:3]  # Normalize by depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02707480",
   "metadata": {},
   "source": [
    "important vocabulary of the paper:\n",
    "- Projection: Rendering 3D point cloud into 2D views using virtual cameras.\n",
    "- Multi-view: Using several camera angles to get a more complete understanding.\n",
    "- In code: Simulated with Open3D or PyTorch3D, capturing rasterized 2D images from different viewpoints.\n",
    "- Purpose: Feed these 2D views into powerful 2D ViTs, then decode back to 3D — enabling masked self-supervised learning in 3D.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6782fef",
   "metadata": {},
   "source": [
    "### Depth map projection phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f9e5d2",
   "metadata": {},
   "source": [
    "$?$  how we project 3D point clouds to multi-view 2D depth images?\n",
    "\n",
    "#### From 3D to 2D via Perspective Projection\n",
    "\n",
    "We want to render a depth map from a point cloud by simulating how a virtual camera would view the scene from multiple viewpoints. This is a classic perspective projection task.\n",
    "\n",
    "Here’s what happens:\n",
    "\n",
    "1. Define Camera Intrinsics and Extrinsics\n",
    "\t- Intrinsic matrix K defines the camera’s internal parameters: focal length, principal point, etc.\n",
    "   - Extrinsic matrix [R|t] defines the camera’s position and orientation in space.\n",
    "\n",
    "2. Transform 3D point cloud into camera coordinates\n",
    "\t- Apply the extrinsic matrix to bring the point cloud into the camera’s local coordinate frame.\n",
    "\n",
    "3. Project onto 2D image plane\n",
    "\t- Use the intrinsic matrix to project the 3D coordinates to 2D.\n",
    "\t- The z coordinate after transformation is used as the depth at that 2D pixel.\n",
    "\n",
    "Let’s say a 3D point is $P = [X, Y, Z, 1]^T$, and we have:\n",
    " - Extrinsic matrix: $E = [R | t] \\in \\mathbb{R}^{3 \\times 4}$\n",
    " - Intrinsic matrix: $K \\in \\mathbb{R}^{3 \\times 3}$\n",
    "\n",
    "Then,\n",
    "$$P_{\\text{cam}} = R \\cdot P_{3D} + t$$\n",
    "$$p_{\\text{img}} = K \\cdot P_{\\text{cam}} \\quad \\text{(homogeneous coords)}$$\n",
    "$$(u, v) = \\left(\\frac{x}{z}, \\frac{y}{z}\\right), \\quad \\text{depth} = z$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feb541e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example code for this phase -> 3.2 3D to multi-view projection and encoding\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_camera_matrix(fx, fy, cx, cy):\n",
    "    return np.array([\n",
    "        [fx, 0, cx],\n",
    "        [0, fy, cy],\n",
    "        [0,  0,  1]\n",
    "    ])\n",
    "\n",
    "def project_point_cloud_to_depth_map(points, intrinsic, extrinsic, H, W):\n",
    "    \"\"\"\n",
    "    points: (N, 3) numpy array of 3D points\n",
    "    intrinsic: (3, 3) camera intrinsic matrix\n",
    "    extrinsic: (4, 4) camera extrinsic matrix\n",
    "    H, W: height and width of output depth map\n",
    "    \"\"\"\n",
    "    N = points.shape[0]\n",
    "    \n",
    "    # Convert to homogeneous coordinates\n",
    "    points_hom = np.concatenate([points, np.ones((N, 1))], axis=1).T  # (4, N)\n",
    "\n",
    "    # Transform to camera coordinates\n",
    "    cam_coords = extrinsic @ points_hom  # (4, N)\n",
    "    cam_coords = cam_coords[:3, :]  # (3, N)\n",
    "\n",
    "    # Project to 2D\n",
    "    pixels = intrinsic @ cam_coords  # (3, N)\n",
    "    pixels = pixels / pixels[2, :]  # Normalize by depth\n",
    "\n",
    "    u = np.round(pixels[0, :]).astype(int)\n",
    "    v = np.round(pixels[1, :]).astype(int)\n",
    "    z = cam_coords[2, :]\n",
    "\n",
    "    # Create depth map\n",
    "    depth_map = np.zeros((H, W), dtype=np.float32)\n",
    "    for i in range(N):\n",
    "        x, y = u[i], v[i]\n",
    "        if 0 <= x < W and 0 <= y < H:\n",
    "            if depth_map[y, x] == 0 or z[i] < depth_map[y, x]:  # Take nearest depth\n",
    "                depth_map[y, x] = z[i]\n",
    "    return depth_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81f605f",
   "metadata": {},
   "source": [
    "part segmentation with Point-MAE:\n",
    "\n",
    "<img src=./images/part_segmenation_Point-MAE.png width=650>\n",
    "\n",
    "in upsampling part, it uses **coordinate-based interpolation** (likely nearest-neighbor or k-NN) to up-sample these sparse features back to each individual point. This is how it bridges patch-level representation to dense per-point prediction.\n",
    "the upsampling part in Point-MAE is similar to image segmentation, where a low-resolution feature map (e.g., 1/8th the image) is upsampled back to full resolution (e.g., *via bilinear interpolation or transposed convs*).\n",
    "\n",
    "the most used upsampling is based on **Inverse Distance Weighted Interpolation**:\n",
    "\n",
    "Use the 3 or 4 nearest center points $c_1, c_2, …, c_k$ and blend their features based on distance:\n",
    "\n",
    "$$f(p_i) = \\frac{\\sum_{j=1}^k w_j f(c_j)}{\\sum_{j=1}^k w_j} \\quad \\text{where } w_j = \\frac{1}{\\|p_i - c_j\\| + \\epsilon}$$\n",
    "\n",
    "after this interpolation, some models (e.g PointNet++) use learnable MLPs after interpolation:\n",
    "\n",
    "$$f{\\prime}(p_i) = \\text{MLP}(f(p_i), p_i)$$\n",
    "\n",
    "This lets the network refine interpolated features with positional information. (learnable MLP which can learn from local and global features --> both from Avg/Max-pooling wigh global class token and Up-sampled tokens from encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863953d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturePropagationMLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, out_dim)\n",
    "        )\n",
    "\n",
    "    # interpolated_feats --> per-point features from upsampled point cloud\n",
    "    # concatenated by coords --> 3D coordinates of the point cloud\n",
    "    # this way, our MLP is able to learn from both local and global features\n",
    "    # and can be used to predict the final features for each point in the point cloud\n",
    "    # in_dim = D + 3, out_dim = D\n",
    "    # where D is the number of features per point in the point cloud\n",
    "    # and 3 is the number of coordinates (x, y, z)\n",
    "    def forward(self, interpolated_feats, coords):\n",
    "        x = torch.cat([interpolated_feats, coords], dim=-1)  # (N, D+3)\n",
    "        return self.mlp(x) # this now can learn from local and global features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071bbc26",
   "metadata": {},
   "source": [
    "$?$ Why Use MLP Here?\n",
    "1.\tLearn geometry-aware feature mappings\n",
    "    - Nearby points might have different meanings depending on object shape.\n",
    "2.\tAdd positional context\n",
    "    - Raw coordinates + interpolated features helps localize features better.\n",
    "3.\tHandle complex patterns\n",
    "    - MLP can learn to denoise, sharpen, or smooth features smartly.\n",
    "\n",
    "*so it Makes upsampling learnable, adaptive, and context-aware*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e92604",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
