{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da5198da",
   "metadata": {},
   "source": [
    "### Masked Autoencoders for 3D point cloud self-supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c292065",
   "metadata": {},
   "source": [
    "##### high-level Notes on the concept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec7cbf4",
   "metadata": {},
   "source": [
    "It proposes a BERT-style pre-training strategy by\n",
    "masking input tokens of the point cloud and then adopts a transformer architecture to predict discrete tokens of the masked tokens.\n",
    "However, this method is relatively sophisticated as it is required to\n",
    "train a DGCNN-based discrete Variational AutoEncoder(dVAE)\n",
    "before pre-training and relies heavily on contrastive learning as well\n",
    "as data augmentation during pre-training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08bf2fe",
   "metadata": {},
   "source": [
    "Moreover, the masked\n",
    "tokens from their inputs are processed from the input of transformer during pre-training, leading to early leakage of location information\n",
    "and high consumption of computing resources. Diﬀerent from their\n",
    "method, and more importantly, to introduce masked autoencoding\n",
    "to the point cloud, we aim to design a neat and eﬃcient scheme of\n",
    "masked autoencoders. To this end, we first analyse the main chal-\n",
    "lenges of introducing masked autoencoding for point cloud from the\n",
    "following aspects:\n",
    "\n",
    "1. The first is the lack of a unified transformer architecture. Com-\n",
    "pared to transformer in NLP and Vision Transformer (ViT)\n",
    "in computervision, transformerarchitectures forpointcloud are\n",
    "lessstudiedandrelatively diverse,mainlybecausesmalldatasets\n",
    "cannot meet the large data demand of transformer. Diﬀerent\n",
    "from previous methods that use dedicated transformer or adopt\n",
    "extra non-transformer models to assist (such as **Point-BERT**\n",
    "uses an extra **DGCNN**), we aim to build our autoencoder’s\n",
    "backbone entirely based on standard transformer, which can\n",
    "serve as a potential unified architecture for point cloud. This\n",
    "also enables further development for point cloud to join general\n",
    "multi-modality frameworks, such as **Data2vec**.\n",
    "\n",
    "2. Positional embeddings for mask tokens lead to **leakage of location** information. In masked autoencoders, each masked part\n",
    "is replaced by a share-weighted learnable mask token. All the\n",
    "mask tokens need to be provided with their location information in input data by positional embeddings. Then after\n",
    "processing by autoencoders, each mask token is used to reconstruct the corresponding masked part. Providing location information is not an issue for languages and images because they\n",
    "do not contain location information. While point cloud naturally has location information in the data, leakage of location\n",
    "information to mask tokens makes the reconstruction task less\n",
    "challenging, which is harmful for autoencoders learning latent\n",
    "features. We address this issue by shifting mask tokens from the\n",
    "input of the autoencoder’s encoder to the input of the autoencoder’s decoder. This delays the leakage of location information and enables the encoder to focus on learning features from\n",
    "unmasked parts.\n",
    "\n",
    "3. Point cloud carries information in a diﬀerent density compared to languages and images. Languages contain high-density information, while images contain heavy redundant informa-\n",
    "tion.12 In the point cloud, information density distribution is\n",
    "relatively uneven. The points that make up key local features\n",
    "(e.g., sharp corners and edges) contain a much higher den-\n",
    "sity of information than the points that make up less impor-\n",
    "tant local features (e.g., flat surfaces). In other words, if being\n",
    "masked, the points that contain high-density information are\n",
    "more diﬃcult to be recovered in the reconstruction task. This\n",
    "can be directly observed in reconstruction examples, as shown\n",
    "in Figure 2. Taking the last row of Figure 2 for illustration,\n",
    "the masked desk surface (left) can be easily recovered, while\n",
    "the reconstruction of the masked motorcycle’s wheel (right) is\n",
    "much worse. Although the point cloud contains uneven density\n",
    "of information, we find that random masking at a high ratio\n",
    "(60–80%) works well, which is surprisingly the same as images.\n",
    "This indicates the point cloud is similar to images instead of\n",
    "languages, in terms of information density."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ca8141",
   "metadata": {},
   "source": [
    "<img src=./images/Point_MAE.png width=650 style=\"display:block; margin:auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b9bea7",
   "metadata": {},
   "source": [
    "**FPS**:\n",
    "The point cloud is processed using FPS (Farthest Point Sampling), a technique to select a subset of points (called centers) that are well-distributed across the point cloud. This reduces the number of points while preserving the overall structure\n",
    "\n",
    "**KNN**: Using KNN (K-Nearest Neighbors), the point cloud is divided into point patches. Each patch is a group of points centered around one of the FPS-selected centers. For example, if a center point is selected, KNN finds its K nearest neighbors in the point cloud, forming a small local patch of points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aa2e03",
   "metadata": {},
   "source": [
    "After the decoder, a **one-layer prediction head** (a simple neural network layer) is used to map the decoder’s output to the desired format.\n",
    "The output of the prediction head is a prediction (Pred) of the masked point patches. Specifically, it predicts the coordinates or features of the points in the masked patches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf54c58e",
   "metadata": {},
   "source": [
    "The loss function used is the **Chamfer Distance (CD)** with an ${l}_2$ norm:\n",
    "Chamfer Distance measures the distance between two point sets (predicted and ground truth) by finding the nearest point correspondences.\n",
    "The $l_2$ norm ensures the predicted points are as close as possible to the ground truth points in terms of Euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be0577c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pytorch3d.loss import chamfer_distance\n",
    "from pytorch3d.ops import sample_farthest_points, knn_points\n",
    "\n",
    "# Hyperparameters\n",
    "NUM_POINTS = 2048  # Total points in the point cloud\n",
    "NUM_PATCHES = 128  # Number of patches\n",
    "POINTS_PER_PATCH = 16  # Points per patch (NUM_POINTS / NUM_PATCHES)\n",
    "MASK_RATIO = 0.8  # Mask 80% of patches\n",
    "EMBED_DIM = 64  # Embedding dimension for tokens\n",
    "NUM_HEADS = 4  # Number of attention heads in Transformer\n",
    "NUM_LAYERS = 2  # Number of Transformer layers\n",
    "\n",
    "class PointMAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PointMAE, self).__init__()\n",
    "        \n",
    "        # Token embedding layer for point patches\n",
    "        self.patch_embed = nn.Linear(POINTS_PER_PATCH * 3, EMBED_DIM)\n",
    "        \n",
    "        # Positional encoding for patch centers\n",
    "        self.pos_embed = nn.Linear(3, EMBED_DIM)\n",
    "        \n",
    "        # Mask token (learnable parameter)\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, EMBED_DIM))\n",
    "        \n",
    "        # Encoder: Transformer for visible tokens\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=EMBED_DIM, nhead=NUM_HEADS)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=NUM_LAYERS)\n",
    "        \n",
    "        # Decoder: Lightweight Transformer\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=EMBED_DIM, nhead=NUM_HEADS)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=1)\n",
    "        \n",
    "        # Prediction head to reconstruct point patches\n",
    "        self.pred_head = nn.Linear(EMBED_DIM, POINTS_PER_PATCH * 3)\n",
    "\n",
    "    def patchify(self, point_cloud):\n",
    "        \"\"\"\n",
    "        Divide point cloud into patches using FPS and KNN.\n",
    "        Input: point_cloud (batch_size, num_points, 3)\n",
    "        Output: patches (batch_size, num_patches, points_per_patch, 3), centers (batch_size, num_patches, 3)\n",
    "        \"\"\"\n",
    "        batch_size = point_cloud.shape[0]\n",
    "        \n",
    "        # FPS to select patch centers\n",
    "        centers, _ = sample_farthest_points(point_cloud, K=NUM_PATCHES)\n",
    "        \n",
    "        # KNN to group points into patches\n",
    "        _, idx, _ = knn_points(centers, point_cloud, K=POINTS_PER_PATCH)\n",
    "        \n",
    "        # Gather points for each patch\n",
    "        patches = torch.gather(\n",
    "            point_cloud.unsqueeze(1).expand(-1, NUM_PATCHES, -1, -1),\n",
    "            2,\n",
    "            idx.unsqueeze(-1).expand(-1, -1, -1, 3)\n",
    "        )  # (batch_size, num_patches, points_per_patch, 3)\n",
    "        \n",
    "        return patches, centers\n",
    "\n",
    "    def mask_patches(self, patches):\n",
    "        \"\"\"\n",
    "        Randomly mask patches.\n",
    "        Input: patches (batch_size, num_patches, points_per_patch, 3)\n",
    "        Output: visible_patches, mask (binary mask indicating masked patches)\n",
    "        \"\"\"\n",
    "        batch_size = patches.shape[0]\n",
    "        num_patches = patches.shape[1]\n",
    "        num_masked = int(num_patches * MASK_RATIO)\n",
    "        \n",
    "        # Randomly shuffle patch indices\n",
    "        idx = torch.rand(batch_size, num_patches, device=patches.device).argsort(dim=1)\n",
    "        mask = torch.ones(batch_size, num_patches, device=patches.device, dtype=torch.bool)\n",
    "        mask.scatter_(1, idx[:, :num_masked], 0)  # Set masked patches to 0\n",
    "        \n",
    "        return mask\n",
    "\n",
    "    def forward(self, point_cloud):\n",
    "        \"\"\"\n",
    "        Forward pass of Point-MAE.\n",
    "        Input: point_cloud (batch_size, num_points, 3)\n",
    "        Output: reconstructed patches, loss\n",
    "        \"\"\"\n",
    "        batch_size = point_cloud.shape[0]\n",
    "        \n",
    "        # Step 1: Patchify the point cloud\n",
    "        patches, centers = self.patchify(point_cloud)  # (batch_size, num_patches, points_per_patch, 3)\n",
    "        \n",
    "        # Step 2: Mask patches\n",
    "        mask = self.mask_patches(patches)  # (batch_size, num_patches)\n",
    "        \n",
    "        # Step 3: Embed patches into tokens\n",
    "        patches_flat = patches.view(batch_size, NUM_PATCHES, -1)  # (batch_size, num_patches, points_per_patch * 3)\n",
    "        tokens = self.patch_embed(patches_flat)  # (batch_size, num_patches, embed_dim)\n",
    "        \n",
    "        # Add positional encoding based on patch centers\n",
    "        pos_enc = self.pos_embed(centers)  # (batch_size, num_patches, embed_dim)\n",
    "        tokens = tokens + pos_enc\n",
    "        \n",
    "        # Step 4: Separate visible tokens for encoder\n",
    "        visible_mask = mask.unsqueeze(-1).expand(-1, -1, EMBED_DIM)  # (batch_size, num_patches, embed_dim)\n",
    "        visible_tokens = tokens * visible_mask  # Zero out masked tokens\n",
    "        visible_tokens = visible_tokens[mask.bool()].view(batch_size, -1, EMBED_DIM)  # (batch_size, num_visible, embed_dim)\n",
    "        \n",
    "        # Step 5: Encoder processes visible tokens\n",
    "        encoded = self.encoder(visible_tokens.transpose(0, 1)).transpose(0, 1)  # (batch_size, num_visible, embed_dim)\n",
    "        \n",
    "        # Step 6: Prepare decoder input (reinsert mask tokens)\n",
    "        decoder_input = tokens.clone()\n",
    "        mask_idx = (mask == 0).unsqueeze(-1).expand(-1, -1, EMBED_DIM)\n",
    "        decoder_input[mask_idx] = self.mask_token.expand(batch_size, NUM_PATCHES, -1)[mask_idx]\n",
    "        \n",
    "        # Step 7: Decoder reconstructs masked patches\n",
    "        decoded = self.decoder(\n",
    "            tgt=decoder_input.transpose(0, 1),\n",
    "            memory=encoded.transpose(0, 1)\n",
    "        ).transpose(0, 1)  # (batch_size, num_patches, embed_dim)\n",
    "        \n",
    "        # Step 8: Prediction head reconstructs patches\n",
    "        recon_patches = self.pred_head(decoded)  # (batch_size, num_patches, points_per_patch * 3)\n",
    "        recon_patches = recon_patches.view(batch_size, NUM_PATCHES, POINTS_PER_PATCH, 3)\n",
    "        \n",
    "        # Step 9: Compute Chamfer Distance loss (only for masked patches)\n",
    "        masked_patches = patches[mask == 0]  # Ground truth masked patches\n",
    "        recon_masked = recon_patches[mask == 0]  # Reconstructed masked patches\n",
    "        if masked_patches.shape[0] > 0:  # Ensure there are masked patches\n",
    "            loss, _ = chamfer_distance(recon_masked, masked_patches)\n",
    "        else:\n",
    "            loss = torch.tensor(0.0, device=point_cloud.device)\n",
    "        \n",
    "        return recon_patches, loss\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Dummy point cloud (batch_size, num_points, 3)\n",
    "    point_cloud = torch.rand(2, NUM_POINTS, 3)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = PointMAE()\n",
    "    \n",
    "    # Forward pass\n",
    "    recon_patches, loss = model(point_cloud)\n",
    "    print(f\"Chamfer Distance Loss: {loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
