{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fec93a8c",
   "metadata": {},
   "source": [
    "#### Neural Radiance Field in 3D computer vision\n",
    "\n",
    "#### my paper implementation file can be found  [here](../src/NeRf/paper_implementation.py)\n",
    "\n",
    "##### [Survey](https://arxiv.org/pdf/2210.00379)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a6ad35",
   "metadata": {},
   "source": [
    "##### Volume Rendering: \n",
    "\n",
    "Volume rendering is a technique used to create 2D images from 3D volumetric data — where “volumetric data” means that inside the space (not just on surfaces), there are properties like *color* and *density* at every point.\n",
    "\n",
    "The core idea is:\n",
    "- Shoot a ray from the camera into the scene.\n",
    "- As you move along the ray, sample points in the volume.\n",
    "- At each sample:\n",
    "    - Query the color and density.\n",
    "    - Accumulate (integrate) these along the ray to compute the final pixel color.\n",
    "\n",
    "the formula of the color $C(r)$ along the ray $r$ is:\n",
    "$$C(r) = \\int_{t_n}^{t_f} T(t) \\sigma(r(t)) \\mathbf{c}(r(t), \\mathbf{d}) \\, dt$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{c}(r(t), \\mathbf{d})$ = RGB color emitted at point $r(t)$ (in the direction $\\mathbf{d})$,\n",
    "- $\\sigma(r(t)$) = density at point $r(t)$ (how much it blocks or emits light),\n",
    "- $T(t)$ = transmittance: how much light from $r(t)$ reaches the camera without getting absorbed.\n",
    "- $t_n$, $t_f$ = near and far bounds along the ray.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222f11d3",
   "metadata": {},
   "source": [
    "----\n",
    "##### from the seurvey:\n",
    "\n",
    "Broadly speaking, novel view synthesis using a trained NeRF model is as follows.\n",
    "\n",
    "- For each pixel in the image being synthesized, send camera rays through the scene and generate a set of sampling points.\n",
    "- For each sampling point, use the viewing direction and sampling location to compute local color and density using the NeRF MLP(s).\n",
    "- Use volume rendering to produce the image from these colors and densities.\n",
    "\n",
    "Given the volume density and color functions of the scene being rendered, volume rendering [21] is used to obtain the position and color $C(\\mathbf{r})$ of any camera ray $\\mathbf{r}(t) = \\mathbf{o} + t\\mathbf{d}$, with camera position $\\mathbf{o}$ and viewing direction $\\mathbf{d}$ using\n",
    "\n",
    "$$C(\\mathbf{r}) = \\int_{t_1}^{t_2} T(t) \\cdot \\sigma(\\mathbf{r}(t)) \\cdot \\mathbf{c}(\\mathbf{r}(t), \\mathbf{d}) \\, dt,$$\n",
    "\n",
    "where $\\sigma(\\mathbf{r}(t))$ and $\\mathbf{c}(\\mathbf{r}(t), \\mathbf{d})$ represent the volume density and color at point $\\mathbf{r}(t)$ along the camera ray with viewing direction $\\mathbf{d}$, and $dt$ represents the differential distance traveled by the ray at each integration step. $T(t)$ is the accumulated transmittance, representing the probability that the ray travels from $t_1$ to $t$ without being intercepted, given by\n",
    "\n",
    "$$T(t) = \\exp\\left(-\\int_{t_1}^{t} \\sigma(\\mathbf{r}(u)) \\, du\\right).$$\n",
    "\n",
    "Novel views are rendered by tracing the camera rays $C(\\mathbf{r})$ through each pixel of the to-be-synthesized image. This integration can be computed numerically. The original implementation [1] and most subsequent methods used a non-deterministic stratified sampling approach, where the ray was divided into $N$ equally spaced bins, and a sample was uniformly drawn from each bin. Then, equation (2) can be approximated as\n",
    "\n",
    "$$\\hat{C}(\\mathbf{r}) = \\sum_{i=1}^{N} \\alpha_i T_i \\mathbf{c}_i, \\quad \\text{where} \\quad T_i = \\exp\\left(-\\sum_{j=1}^{i-1} \\sigma_j \\delta_j\\right),$$\n",
    "\n",
    "$\\delta_i$ is the distance from sample $i$ to sample $i+1$, $(\\sigma_i, \\mathbf{c}_i)$ are the density and color evaluated along the sample point $i$ given the ray, as computed by the NeRF MLP(s). $\\alpha_i$ the transparency/opacity from alpha compositing at sample point $i$, is given by\n",
    "\n",
    "$$\\alpha_i = 1 - \\exp(-\\sigma_i \\delta_i).$$\n",
    "\n",
    "An expected depth can be calculated for the ray using the accumulated transmittance as\n",
    "\n",
    "$$d(\\mathbf{r}) = \\int_{t_1}^{t_2} T(t) \\cdot \\sigma(\\mathbf{r}(t)) \\cdot t \\, dt.$$\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02958d78",
   "metadata": {},
   "source": [
    "<img src=./images/NeRF.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c179d7e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
